Encoding(keyword)
keyword2 <- URLencode(keyword)
keyword2
url <- paste0("https://m.map.naver.com/search2/searchMore.nhn?query=",keyword2,"&sm=clk&style=v5&page=1&displayCount=75&type=SITE_1" )
url
b <- readLines(url, encoding = "UTF-8")
library(RJSONIO)
b2<- fromJSON(b)
b2
url <- paste0("https://m.map.naver.com/search2/searchMore.nhn?query=",keyword2,"&sm=clk&style=v5&page=1&displayCount=75&type=SITE_1" )
b <- readLines(url, encoding = "UTF-8")
library(RJSONIO)
b2<- fromJSON(b)
b2
keyword <- "신촌카페"
keyword <- iconv(keyword, from="CP949",to="UTF-8")
Encoding(keyword)
keyword2 <- URLencode(keyword)
url <- paste0("https://m.map.naver.com/search2/searchMore.nhn?query=",keyword2,"&sm=clk&style=v5&page=1&displayCount=75&type=SITE_1" )
b <- readLines(url, encoding = "UTF-8")
b2<- fromJSON(b)
b2
b2$meta
b2$ac
b2$poi
b <- paste(b,collapse = " ")
b2<- fromJSON(b)
b2$result
b2$result$site
b2$result$site$list[[1]]
b2$result$site$list[[1]]$name
b2$result$site$list[[1]]$id
sapply(b2$b2$result$site$list,function(x){x$name} )
sapply(b2$result$site$list,function(x){x$name} )
sapply(b2$result$site$list,function(x){x$address} )
name <- sapply(b2$result$site$list,function(x){x$name} )
id <- sapply(b2$result$site$list,function(x){x$id} )
x <- sapply(b2$result$site$list,function(x){x$x} )
y <- sapply(b2$result$site$list,function(x){x$y} )
adress <- sapply(b2$result$site$list,function(x){x$address} )
addr <- sapply(b2$result$site$list,function(x){x$address} )
cbind(name, id, x, y, addr)
data <- cbind(name, id, x, y, addr)
write.csv(data, paste0("keyword",".csv"),row.names = F)
data <- cbind(name, id, x, y, addr)
write.csv(data, paste0("keyword",".csv"),row.names = F)
write.csv(data, paste0(keyword,".csv"),row.names = F)
b <- readLines(url, encoding="UTF-8")
b <- readLines(url, encoding = "UTF-8")
url <- "https://sports.news.naver.com/kbaseball/news/list.nhn?isphoto=N&date=20200401"
b <- readLines(url, encoding = "UTF-8")
b
library(RJSONIO)
b
fromJSON(b) -> b2
b2
b2$list[[1]]
b2$list
b2$dat
b2$list[[1]]
sapply(b2$list,function(x){x$oid})
sapply(b2$list,function(x){x$aid})
sapply(b2$list,function(x){x$title})
url <- "https://sports.news.naver.com/kbaseball/news/list.nhn?isphoto=N&date=20200401&page=2"
b <- readLines(url, encoding = "UTF-8")
fromJSON(b) -> b2
b2$list[[1]]
sapply(b2$list,function(x){x$oid})
sapply(b2$list,function(x){x$aid})
sapply(b2$list,function(x){x$title})
for(i in 1:2){
url <- paste("https://sports.news.naver.com/kbaseball/news/list.nhn?isphoto=N&date=20200401&page=",i)
b <- readLines(url, encoding = "UTF-8")
fromJSON(b) -> b2
sapply(b2$list,function(x){x$oid})
sapply(b2$list,function(x){x$aid})
sapply(b2$list,function(x){x$title})
}
for(i in 1:2){
url <- paste("https://sports.news.naver.com/kbaseball/news/list.nhn?isphoto=N&date=20200401&page=",i)
b <- readLines(url, encoding = "UTF-8")
fromJSON(b) -> b2
sapply(b2$list,function(x){x$oid})
sapply(b2$list,function(x){x$aid})
print(sapply(b2$list,function(x){x$title}))
}
date <- Sys.Date()
data
date
date <- Sys.Date()
date2 <-gsub("-","",date)
for(j in 1:5){
for(i in 1:2){
date <- Sys.Date() -j
date2 <-gsub("-","",date)
url <- paste("https://sports.news.naver.com/kbaseball/news/list.nhn?isphoto=N&date=",2020040,"1&page=",i)
b <- readLines(url, encoding = "UTF-8")
fromJSON(b) -> b2
a1<-sapply(b2$list,function(x){x$oid})
a2<-sapply(b2$list,function(x){x$aid})
a3<-sapply(b2$list,function(x){x$title})
final_data<- rbind(final_data, c(a1,a2,a3))
cat("\n",date2,"-",i,j)
}
}
for(j in 1:5){
for(i in 1:2){
date <- Sys.Date() -j
date2 <-gsub("-","",date)
url <- paste("https://sports.news.naver.com/kbaseball/news/list.nhn?isphoto=N&date=",j,"&page=",i)
b <- readLines(url, encoding = "UTF-8")
fromJSON(b) -> b2
a1<-sapply(b2$list,function(x){x$oid})
a2<-sapply(b2$list,function(x){x$aid})
a3<-sapply(b2$list,function(x){x$title})
final_data<- rbind(final_data, c(a1,a2,a3))
cat("\n",date2,"-",i,j)
}
}
for(j in 1:5){
for(i in 1:2){
date <- Sys.Date() -j
date2 <-gsub("-","",date)
url <- paste("https://sports.news.naver.com/kbaseball/news/list.nhn?isphoto=N&date=",date2,"&page=",i)
b <- readLines(url, encoding = "UTF-8")
fromJSON(b) -> b2
a1<-sapply(b2$list,function(x){x$oid})
a2<-sapply(b2$list,function(x){x$aid})
a3<-sapply(b2$list,function(x){x$title})
final_data<- rbind(final_data, c(a1,a2,a3))
cat("\n",date2,"-",i,j)
}
}
date2
for(i in 1:2){
date <- Sys.Date() -j
date2 <-gsub("-","",date)
url <- paste0("https://sports.news.naver.com/kbaseball/news/list.nhn?isphoto=N&date=",date2,"&page=",i)
b <- readLines(url, encoding = "UTF-8")
fromJSON(b) -> b2
a1<-sapply(b2$list,function(x){x$oid})
a2<-sapply(b2$list,function(x){x$aid})
a3<-sapply(b2$list,function(x){x$title})
final_data<- rbind(final_data, c(a1,a2,a3))
cat("\n",date2,"-",i,j)
}
i <-1
j <-1
final_data <- NULL
for(j in 1:5){
for(i in 1:2){
date <- Sys.Date() -j
date2 <-gsub("-","",date)
url <- paste0("https://sports.news.naver.com/kbaseball/news/list.nhn?isphoto=N&date=",date2,"&page=",i)
b <- readLines(url, encoding = "UTF-8")
fromJSON(b) -> b2
a1<-sapply(b2$list,function(x){x$oid})
a2<-sapply(b2$list,function(x){x$aid})
a3<-sapply(b2$list,function(x){x$title})
final_data<- rbind(final_data, c(a1,a2,a3))
cat("\n",date2,"-",i,j)
}
}
i <-1
j <-1
final_data <- NULL
for(j in 1:5){
for(i in 1:2){
date <- Sys.Date() -j
date2 <-gsub("-","",date)
url <- paste0("https://sports.news.naver.com/kbaseball/news/list.nhn?isphoto=N&date=",date2,"&page=",i)
b <- readLines(url, encoding = "UTF-8")
fromJSON(b) -> b2
a1<-sapply(b2$list,function(x){x$oid})
a2<-sapply(b2$list,function(x){x$aid})
a3<-sapply(b2$list,function(x){x$title})
final_data<- rbind(final_data, c(a1,a2,a3))
cat("\n",date2,"-",j,i)
}
}
final_data
write.csv(final_data,"baseball_news.csv",row.names=F)
i <-1
j <-1
final_data <- NULL
date <- Sys.Date() -j
date2 <-gsub("-","",date)
url <- paste0("https://sports.news.naver.com/kbaseball/news/list.nhn?isphoto=N&date=",date2,"&page=",i)
b <- readLines(url, encoding = "UTF-8")
fromJSON(b) -> b2
a1<-sapply(b2$list,function(x){x$oid})
a2<-sapply(b2$list,function(x){x$aid})
a3<-sapply(b2$list,function(x){x$title})
a1
c(a1,a2,a3)
cbind(a1,a2,a3)
i <-1
j <-1
final_data <- NULL
for(j in 1:5){
for(i in 1:2){
date <- Sys.Date() -j
date2 <-gsub("-","",date)
url <- paste0("https://sports.news.naver.com/kbaseball/news/list.nhn?isphoto=N&date=",date2,"&page=",i)
b <- readLines(url, encoding = "UTF-8")
fromJSON(b) -> b2
a1<-sapply(b2$list,function(x){x$oid})
a2<-sapply(b2$list,function(x){x$aid})
a3<-sapply(b2$list,function(x){x$title})
final_data<- rbind(final_data, cbind(a1,a2,a3))
cat("\n",date2,"-",j,i)
}
}
final_data
write.csv(final_data,"baseball_news.csv",row.names=F)
write.csv(final_data,"baseball_news.csv",row.names=F)
con_url <- paste0("https://sports.news.naver.com/news.nhn?oid=",final_data[,1],"&aid=",final_data[,2])
con_url
library(stringr)
k <- 1
b<-readLines(con_url[k],encoding = "UTF-8")
str_detect(b,"id=\"newsEndContents\">")
which(str_detect(b,"id=\"newsEndContents\">"))
b[which(str_detect(b,"id=\"newsEndContents\">"))]
which(str_detect(b,"news_end_btn"))
b2<-b[which(str_detect(b,"id=\"newsEndContents\">")): which(str_detect(b,"news_end_btn"))]
b2
b3<-paste(b2,collapse = " ")
b3
b3 <- gsub("<.*?>",",b3")
b3 <- gsub("<.*?>","",b3)
b3
b3 <- gsub("\t|&gt;|&#160","",b3)
b3
b3 <- gsub("\t|&gt;|&#160|&lt;","",b3)
b3
con <- c()
for(k in 1:seq_along(con_url)){
b<-readLines(con_url[k],encoding = "UTF-8")
b2<-b[which(str_detect(b,"id=\"newsEndContents\">")): which(str_detect(b,"news_end_btn"))]
b3<-paste(b2,collapse = " ")
b3 <- gsub("<.*?>","",b3)
b3 <- gsub("\t|&gt;|&#160|&lt;","",b3)
con <- c(con,b3)
}
con
for(k in 1:seq_along(con_url)){
b<-readLines(con_url[k],encoding = "UTF-8")
b2<-b[which(str_detect(b,"id=\"newsEndContents\">")): which(str_detect(b,"news_end_btn"))]
b3<-paste(b2,collapse = " ")
b3 <- gsub("<.*?>","",b3)
b3 <- gsub("\t|&gt;|&#160|&lt;","",b3)
con <- c(con,b3)
cat("\n",k)
}
for(k in 1:length(con_url))){
b<-readLines(con_url[k],encoding = "UTF-8")
b2<-b[which(str_detect(b,"id=\"newsEndContents\">")): which(str_detect(b,"news_end_btn"))]
b3<-paste(b2,collapse = " ")
b3 <- gsub("<.*?>","",b3)
b3 <- gsub("\t|&gt;|&#160|&lt;","",b3)
con <- c(con,b3)
cat("\n",k)
}
for(k in 1:length(con_url)){
b<-readLines(con_url[k],encoding = "UTF-8")
b2<-b[which(str_detect(b,"id=\"newsEndContents\">")): which(str_detect(b,"news_end_btn"))]
b3<-paste(b2,collapse = " ")
b3 <- gsub("<.*?>","",b3)
b3 <- gsub("\t|&gt;|&#160|&lt;","",b3)
con <- c(con,b3)
cat("\n",k)
}
seq_along(con_url)
for(k in 1:seq_along(con_url)){
b<-readLines(con_url[k],encoding = "UTF-8")
b2<-b[which(str_detect(b,"id=\"newsEndContents\">")): which(str_detect(b,"news_end_btn"))]
b3<-paste(b2,collapse = " ")
b3 <- gsub("<.*?>","",b3)
b3 <- gsub("\t|&gt;|&#160|&lt;","",b3)
con <- c(con,b3)
cat("\n",k)
}
seq_along(con_url)[0]
seq_along(con_url)[1]
seq_along(con_url)[2]
seq_along(con_url)[200]
seq_along(con_url)[201]
k <- 1
con <- c()
for(k in 1:seq_along(con_url)){
b<-readLines(con_url[k],encoding = "UTF-8")
b2<-b[which(str_detect(b,"id=\"newsEndContents\">")): which(str_detect(b,"news_end_btn"))]
b3<-paste(b2,collapse = " ")
b3 <- gsub("<.*?>","",b3)
b3 <- gsub("\t|&gt;|&#160|&lt;","",b3)
con <- c(con,b3)
cat("\n",k)
}
cbind(final_data,con) -> baseball_data
baseball_data
setNames(baseball_data,c("oid","aid","head","cont"))
baseball_data
setNames(baseball_data,c("oid","aid","head","cont"))
setnames(baseball_data,"oid","aid","head","cont")
library(RJSONIO)
library(stringr)
i <-1
j <-1
final_data <- NULL
for(j in 1:5){
for(i in 1:2){
date <- Sys.Date() -j
date2 <-gsub("-","",date)
url <- paste0("https://sports.news.naver.com/kbaseball/news/list.nhn?isphoto=N&date=",date2,"&page=",i)
b <- readLines(url, encoding = "UTF-8")
fromJSON(b) -> b2
a1<-sapply(b2$list,function(x){x$oid})
a2<-sapply(b2$list,function(x){x$aid})
a3<-sapply(b2$list,function(x){x$title})
final_data<- rbind(final_data, cbind(a1,a2,a3))
cat("\n",date2,"-",j,i)
}
}
final_data
write.csv(final_data,"baseball_news.csv",row.names=F)
con_url <- paste0("https://sports.news.naver.com/news.nhn?oid=",final_data[,1],"&aid=",final_data[,2])
con_url
k <- 1
con <- c()
for(k in 1:length(con_url)){
b<-readLines(con_url[k],encoding = "UTF-8")
b2<-b[which(str_detect(b,"id=\"newsEndContents\">")): which(str_detect(b,"news_end_btn"))]
b3<-paste(b2,collapse = " ")
b3 <- gsub("<.*?>","",b3)
b3 <- gsub("\t|&gt;|&#160|&lt;","",b3)
con <- c(con,b3)
cat("\n",k)
}
cbind(final_data,con) -> baseball_data
baseball_data
str(baseball_data)
colnames(baseball_data) <- c("oid","aid","head","cont")
glimpse(baseball_data)
tidyverse::glimpse(bseball_data)
tibble::glimpse(bseball_data)
tibble::glimpse(baseball_data)
write.csv(baseball_data, "baseball.csv",row.names = F)
b <- readLines(url, encoding="UTF-8")
library(stringr)
b[str_detect(b,"class=\"ltl\">")]
b <- readLines(url, encoding="UTF-8")
url <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200405&page=1"
b <- readLines(url, encoding="UTF-8")
library(stringr)
b[str_detect(b,"class=\"ltl\">")]
b[str_detect(b,"class=\"lt1\">")]
b[str_detect(b,"class=\"lt1\">"),14,end=-16]
str_sub(b[str_detect(b,"class=\"lt1\">")],14,end-16)
str_sub(b[str_detect(b,"class=\"lt1\">")],14,end=-16)
paste0("http:/", str_sub(b[str_detect(b,"class=\"lt1\">")],14,end=-16))
nurl <- paste0("http:/", str_sub(b[str_detect(b,"class=\"lt1\">")],14,end=-16))
str_detect(b,"<strong class=\"tit\">")
b[str_detect(b,"<strong class=\"tit\">")]
b <- readLines(url, encoding="euc-kr")
nurl <- paste0("http:/", str_sub(b[str_detect(b,"class=\"lt1\">")],14,end=-16))
b[str_detect(b,"<strong class=\"tit\">")]
b2 <- b[str_detect(b,"<strong class=\"tit\">")]
str_extract(b2, "(?<=tit).*(?=</strong>) ")
str_extract(b2, "(?<=tit).*(?=</strong>)")
str_sub(str_extract(b2, "(?<=tit).*(?=</strong>)"),3,end=-2)
final_url <- c()
final_tit <- c()
j<-0
ddate <- Sys.Date()-j
ddate2 <- gsub("-","",ddate)
ddate <- Sys.Date()-j
ddate2 <- gsub("-","",ddate)
final_url <- c()
final_tit <- c()
j<-0
for(j in 0:5){
ddate <- Sys.Date()-j
ddate2 <- gsub("-","",ddate)
for(i in 1: 5){
url <- paste0("https://news.nate.com/recent?mid=n0100&type=c&date=", ddate2 ,"&page=",i)
b <- readLines(url, encoding="euc-kr")
nurl <- paste0("http:/", str_sub(b[str_detect(b,"class=\"lt1\">")],14,end=-16))
b2 <- b[str_detect(b,"<strong class=\"tit\">")]
str_sub(str_extract(b2, "(?<=tit).*(?=</strong>)"),3,end=-2)
cat("\n",i,"-",j)
}
}
final_url <- c()
final_tit <- c()
j<-0
for(j in 0:5){
ddate <- Sys.Date()-j
ddate2 <- gsub("-","",ddate)
for(i in 1: 5){
url <- paste0("https://news.nate.com/recent?mid=n0100&type=c&date=", ddate2 ,"&page=",i)
b <- readLines(url, encoding="euc-kr")
nurl <- paste0("http:/", str_sub(b[str_detect(b,"class=\"lt1\">")],14,end=-16))
b2 <- b[str_detect(b,"<strong class=\"tit\">")]
tit<-str_sub(str_extract(b2, "(?<=tit).*(?=</strong>)"),3,end=-2)
final_url <- c(final_url, b2)
final_tit <- c(final_tit, tit)
cat("\n",i,"-",j)
}
}
final_url
final_tit
final_url <- c()
final_tit <- c()
j<-0
for(j in 0:5){
ddate <- Sys.Date()-j
ddate2 <- gsub("-","",ddate)
for(i in 1: 5){
url <- paste0("https://news.nate.com/recent?mid=n0100&type=c&date=", ddate2 ,"&page=",i)
b <- readLines(url, encoding="euc-kr")
nurl <- paste0("http:/", str_sub(b[str_detect(b,"class=\"lt1\">")],14,end=-16))
b2 <- b[str_detect(b,"<strong class=\"tit\">")]
tit<-str_sub(str_extract(b2, "(?<=tit).*(?=</strong>)"),3,end=-2)
final_url <- c(final_url, b)
final_tit <- c(final_tit, tit)
cat("\n",i,"-",j)
}
}
final_url
url <- paste0("https://news.nate.com/recent?mid=n0100&type=c&date=", ddate2 ,"&page=",i)
b <- readLines(url, encoding="euc-kr")
nurl <- paste0("http:/", str_sub(b[str_detect(b,"class=\"lt1\">")],14,end=-16))
nurl
for(j in 0:5){
ddate <- Sys.Date()-j
ddate2 <- gsub("-","",ddate)
for(i in 1: 5){
url <- paste0("https://news.nate.com/recent?mid=n0100&type=c&date=", ddate2 ,"&page=",i)
b <- readLines(url, encoding="euc-kr")
nurl <- paste0("http:/", str_sub(b[str_detect(b,"class=\"lt1\">")],14,end=-16))
b2 <- b[str_detect(b,"<strong class=\"tit\">")]
tit<-str_sub(str_extract(b2, "(?<=tit).*(?=</strong>)"),3,end=-2)
final_url <- c(final_url, nurl)
final_tit <- c(final_tit, tit)
cat("\n",i,"-",j)
}
}
k<-1
b <- readLines(final_url[k], encoding = "euc-kr")
final_url[1]
final_url[2]
final_url <- c()
final_tit <- c()
j<-0
for(j in 0:5){
ddate <- Sys.Date()-j
ddate2 <- gsub("-","",ddate)
for(i in 1: 5){
url <- paste0("https://news.nate.com/recent?mid=n0100&type=c&date=", ddate2 ,"&page=",i)
b <- readLines(url, encoding="euc-kr")
nurl <- paste0("http:/", str_sub(b[str_detect(b,"class=\"lt1\">")],14,end=-16))
b2 <- b[str_detect(b,"<strong class=\"tit\">")]
tit<-str_sub(str_extract(b2, "(?<=tit).*(?=</strong>)"),3,end=-2)
final_url <- c(final_url, nurl)
final_tit <- c(final_tit, tit)
cat("\n",i,"-",j)
}
}
k<-1
b <- readLines(final_url[k], encoding = "euc-kr")
which(str_detect(b, "<div id=\"realArtcContents\">"))
which(str_detect(b, "<script language=\"javascript\" type=\"application/javascript\">" ) )
aindex <- which(str_detect(b, "<div id=\"realArtcContents\">"))
bindex <- which(str_detect(b, "<script language=\"javascript\" type=\"application/javascript\">" ) )
b[aindex:bindex]
b2 <- paste( b[aindex:bindex], collapse = " ")
b2
k<-1
for(k in 1:length(final_url)){
b <- readLines(final_url[k], encoding = "euc-kr")
aindex <- which(str_detect(b, "<div id=\"realArtcContents\">"))
bindex <- which(str_detect(b, "<script language=\"javascript\" type=\"application/javascript\">" ) )
b2 <- paste( b[aindex:bindex], collapse = " ")
final_con[k] <- b2
cat("\n",k)
}
k<-1
final_con <- c()
for(k in 1:length(final_url)){
b <- readLines(final_url[k], encoding = "euc-kr")
aindex <- which(str_detect(b, "<div id=\"realArtcContents\">"))
bindex <- which(str_detect(b, "<script language=\"javascript\" type=\"application/javascript\">" ) )
b2 <- paste( b[aindex:bindex], collapse = " ")
final_con[k] <- b2
cat("\n",k)
}
final_con
final_url
install.packages("bupaR")
library(bupaR)
install.packages("edeaR")
install.packages("processmapR")
install.packages("xesreadR")
install.packages("processmonitR")
install.packages("petrinetR")
library(edeaR)
library(processmapR)
library(xeseradR)
library(xesreadR)
library(processmonitR)
library(petrinetR)
